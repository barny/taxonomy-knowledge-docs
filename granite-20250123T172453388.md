| platform = [IBM Watsonx](IBM_Watsonx "wikilink") (initially)  
[GitHub](GitHub "wikilink")  
[Hugging Face](Hugging_Face "wikilink")  
[RHEL](RHEL "wikilink") AI | replaces = | replaced\_by = | license =
[Proprietary](Proprietary_software "wikilink")  
Code models: [Open Source](Open-source_software "wikilink") ([Apache
2.0](Apache_License "wikilink"))[1] }} **IBM Granite** is a series of
decoder-only [AI](AI "wikilink") [foundation
models](foundation_model "wikilink") created by
[IBM](IBM "wikilink").[2] It was announced on September 7, 2023,[3][4]
and an initial paper was published 4 days later.[5] Initially intended
for use in the IBM's [cloud-based](cloud-based "wikilink")
[data](data "wikilink") and [generative AI](generative_AI "wikilink")
platform [Watsonx](Watsonx "wikilink") along with other models,[6] IBM
opened the source code of some code models.[7][8] Granite models are
trained on datasets curated from [Internet](Internet "wikilink"),
[academic publishings](Academic_publishing "wikilink"),
[code](Source_code "wikilink") datasets,
[legal](Legal_instrument "wikilink") and finance documents.[9][10][11]

## Foundation models

A foundation model is an AI model trained on broad data at scale such
that it can be adapted to a wide range of downstream tasks.[12]

Granite's first foundation models were Granite.13b.instruct and
Granite.13b.chat. The "13b" in their name comes from 13 billion, the
amount of [parameters](Large_language_model#Scaling_laws "wikilink")
they have as models, lesser than most of the larger models of the time.
Later models vary from 3 to 34 billion parameters.[13][14]

On May 6, 2024, IBM released the [source code](source_code "wikilink")
of four variations of Granite Code Models under [Apache
2](Apache_License "wikilink"), an open source [permissive
license](Permissive_software_license "wikilink") that allows completely
free use, modification and sharing of the software, and put them on
[Hugging Face](Hugging_Face "wikilink") for public use.[15][16]
According to IBM's own report, Granite 8b outperforms [Llama
3](Llama_3 "wikilink") on several
[coding](Computer_programming "wikilink") related tasks within similar
range of parameters.[17][18]

## See also

-   [Mistral AI](Mistral_AI "wikilink"), a company that also provides
    open source models
-   [GPT](Generative_pre-trained_transformer "wikilink")
-   [LLaMA](LLaMA "wikilink")
-   [Cyc](Cyc "wikilink")
-   [Gemini](Gemini_(language_model) "wikilink")

## References

## External links

-   [GitHub page](https://github.com/ibm-granite)
-   [IBM Granite Playground](https://www.ibm.com/granite/playground/)

[Category:IBM products](Category:IBM_products "wikilink") [Category:IBM
software](Category:IBM_software "wikilink") [Category:Large language
models](Category:Large_language_models "wikilink") [Category:Generative
artificial
intelligence](Category:Generative_artificial_intelligence "wikilink")
[Category:Artificial neural
networks](Category:Artificial_neural_networks "wikilink") [Category:2023
software](Category:2023_software "wikilink") [Category:Free
software](Category:Free_software "wikilink") [Category:Open-source
artificial
intelligence](Category:Open-source_artificial_intelligence "wikilink")

[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]
Synced
|url=<https://syncedreview.com/2024/05/13/ibms-granite-code-powering-enterprise-software-development-with-ai-precision/>
|access-date=2024-05-21 |website=syncedreview.com |language=en-US}}